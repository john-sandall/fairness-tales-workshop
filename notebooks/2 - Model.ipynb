{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://static1.squarespace.com/static/5ba26f9d89c1720405dcfae2/t/5bbc69570d929721d5a5ff2c/1726236705071/\" width=300>\n",
    "\n",
    "<h1>PyData London 2025</h1>\n",
    "<h2>How To Measure And Mitigate Unfair Bias in Machine Learning Models</h2>\n",
    "<h3>Notebook 2 - Modelling</h3>\n",
    "\n",
    "This notebook demonstrates how bias can emerge in machine learning models and explores different techniques to mitigate it. Using a resume screening dataset, we:\n",
    "\n",
    "1. Train an intentionally biased model to show how ML systems can perpetuate discrimination\n",
    "2. Evaluate the model's fairness metrics across different demographic groups\n",
    "3. Apply various fairness-aware techniques (including Fairlearn) to mitigate the discovered biases\n",
    "4. Compare the performance-fairness tradeoffs between different approaches\n",
    "\n",
    "The goal is to illustrate both the potential pitfalls in ML systems and practical approaches to building more equitable models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we'll deliberately create and then fix a biased model. This helps us:\n",
    "1. Understand how bias can emerge in ML systems\n",
    "2. Learn to identify bias through metrics\n",
    "3. Practice different approaches to mitigate unfairness\n",
    "\n",
    "Remember: In a real application, we would never intentionally create a biased model. This is purely for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section 1: Let's train a (very) biased model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Note for Mac users**: LightGBM sometimes can't find the OpenMP library (libomp.dylib) that it depends on for parallel processing. This is a common issue on macOS, especially with Apple Silicon Macs. If you encounter this error with the following step it can be fixed by installing the missing library and reinstall the LightGBM module:\n",
    "\n",
    "```bash\n",
    "brew install libomp\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm --no-binary lightgbm\n",
    "```\n",
    "\n",
    "If you don't have Homebrew, install it first:\n",
    "\n",
    "```bash\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn import dummy, ensemble, model_selection, tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "ROOT = Path()\n",
    "np.set_printoptions(legacy=\"1.25\")\n",
    "\n",
    "# Suppress all FutureWarning messages\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a variety of libraries for this analysis:\n",
    "- `lightgbm` for our main classifier\n",
    "- Standard data science tools (`numpy`, `pandas`, `sklearn`)\n",
    "- `fairlearn` for bias mitigation (we'll use this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_cleaner(s, n=8):\n",
    "    \"\"\"Worse I've seen is a CV full of whitespace, n=8 was enough for this.\"\"\"\n",
    "    for i in range(n, 0, -1):\n",
    "        s = s.replace(\" \" * i, \" \")\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_cv_text(text):\n",
    "    return whitespace_cleaner(\n",
    "        text.lower()\n",
    "        .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        .replace(\"•\", \" \")\n",
    "        .replace(\"\\n\", \" \")\n",
    "        .replace(\"*\", \" \")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"—\", \" \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(ROOT / \"data\" / \"output\" / \"resumes-400.feather\").reset_index(drop=True)\n",
    "\n",
    "df[\"text\"] = df[\"cv\"].apply(clean_cv_text)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df, x=\"sex\", y=\"callback\", hue=\"quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working with resume data where we want to predict whether a candidate received a callback. The data includes the CV text and demographic information like gender.\n",
    "\n",
    "First, we'll clean the CV text by:\n",
    "- Converting to lowercase\n",
    "- Removing punctuation and special characters\n",
    "- Standardizing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Let's build a (very) biased model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"text\"]]\n",
    "y = df[\"callback\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=200, ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "vectorizer.fit(X.text)\n",
    "X_text = pd.DataFrame(\n",
    "    vectorizer.transform(X.text).toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")\n",
    "X = pd.concat([X.drop(columns=\"text\"), X_text], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our text data into features, we use a CountVectorizer to:\n",
    "1. Extract the top 200 most common words/phrases (unigrams and bigrams)\n",
    "2. Remove common English stop words\n",
    "3. Create a sparse matrix where each column represents a word/phrase frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Baseline callback rate is {y.mean():.1%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Let's fit a small decision tree for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=4)\n",
    "model = model.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True, rounded=True, fontsize=9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=y, x=df.sex);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=(X.aws > 0), y=y, hue=df.sex);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple decision tree to visualize how the model makes decisions. We'll intentionally keep it shallow (max_depth=4) for interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Find our next top (biased) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_val_score(\n",
    "    dummy.DummyClassifier(),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_val_score(\n",
    "    ensemble.RandomForestClassifier(n_estimators=100),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_val_score(\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_val_score(\n",
    "    tree.DecisionTreeClassifier(max_depth=4),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 10,\n",
    "    \"max_depth\": 3,\n",
    "    \"n_jobs\": 1,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "model_selection.cross_val_score(\n",
    "    model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interpret the LGBM classifier using permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(model, X, y, n_repeats=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame({\"feature\": X.columns, \"importance\": r[\"importances_mean\"]})\n",
    "    .sort_values(\"importance\")\n",
    "    .set_index(\"feature\")\n",
    "    .tail(10)\n",
    "    .plot(kind=\"barh\")\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=(X[\"architect\"] > 0), y=y, hue=df.sex);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section 2: Export for Aequitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aequitas = y.reset_index().rename(columns={\"callback\": \"label_value\"}).drop(columns=\"index\")\n",
    "aequitas[\"label_value\"] = y\n",
    "aequitas[\"score\"] = model.predict(X)\n",
    "aequitas[\"sex\"] = df.sex\n",
    "\n",
    "aequitas.to_csv(ROOT / \"data\" / \"output\" / \"aequitas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section 3: Intro to Fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairlearn\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    equalized_odds_difference,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    ")\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import EqualizedOdds, ErrorRate, ExponentiatedGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding Fairlearn\n",
    "\n",
    "Fairlearn is a Python package that helps assess and mitigate unfairness in machine learning models. It provides tools for:\n",
    "1. Measuring model fairness through disaggregated metrics\n",
    "2. Mitigating unfairness through various algorithmic interventions\n",
    "3. Visualizing and comparing model performance across different demographic groups\n",
    "\n",
    "In this notebook, we'll explore two main mitigation approaches:\n",
    "- Post-processing with ThresholdOptimizer\n",
    "- In-processing with the Reductions approach\n",
    "\n",
    "Both techniques help us balance model performance with fairness constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding MetricFrame\n",
    "\n",
    "A MetricFrame is Fairlearn's core tool for disaggregated performance assessment. It allows us to:\n",
    "- Evaluate multiple metrics across different demographic groups\n",
    "- Compare model performance between groups\n",
    "- Quantify disparities in model behavior\n",
    "\n",
    "MetricFrame calculates:\n",
    "- Overall metrics (aggregate performance)\n",
    "- By-group metrics (performance for each demographic group)\n",
    "- Difference metrics (largest gap between any two groups)\n",
    "\n",
    "In our case, we're particularly interested in:\n",
    "- Balanced accuracy (model performance)\n",
    "- False positive/negative rates (error patterns)\n",
    "- Equalized odds difference (overall fairness measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepare train, test, and protected characteristics dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_str = df.sex.astype(\"category\")\n",
    "A_str.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, A_train, A_test = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    A_str,\n",
    "    test_size=0.35,\n",
    "    stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fit our LGBM model to the training data & generate predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate the \"raw\" unmitigated LGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metrics = {\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "    \"false_positive_rate\": false_positive_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_unmitigated = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_unmitigated.overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mf_unmitigated.by_group.plot.bar(subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_unmitigated = balanced_accuracy_score(y_test, y_pred)\n",
    "balanced_accuracy_unmitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_unmitigated = equalized_odds_difference(y_test, y_pred, sensitive_features=A_test)\n",
    "equalized_odds_unmitigated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mitigate using post-processing techniques (ThresholdOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Understanding ThresholdOptimizer\n",
    "\n",
    "ThresholdOptimizer is a post-processing technique that adjusts model predictions after training. Key points:\n",
    "- Takes an existing trained model and transforms its outputs\n",
    "- Finds different thresholds for each demographic group\n",
    "- Optimizes for a specified metric (like balanced accuracy) while satisfying fairness constraints\n",
    "- Requires access to sensitive features during both training and prediction\n",
    "\n",
    "In our case, we use it to optimize balanced accuracy while satisfying equalized odds constraints. This means we're trying to:\n",
    "1. Maintain good overall prediction accuracy\n",
    "2. Ensure similar false positive and false negative rates across gender groups\n",
    "\n",
    "Note: A key limitation is that we need the sensitive feature (gender) at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_mitigator = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",  # Optimize FPR and FNR simultaneously\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method=\"predict_proba\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_mitigator.fit(X=X_train, y=y_train, sensitive_features=A_train)\n",
    "y_pred_postprocess = postprocess_mitigator.predict(X_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_postprocess = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_postprocess,\n",
    "    sensitive_features=A_test,\n",
    ")\n",
    "mf_postprocess.overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_postprocess.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_postprocess.by_group.plot.bar(subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_postprocess.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_postprocess = balanced_accuracy_score(y_test, y_pred_postprocess)\n",
    "balanced_accuracy_postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_postprocess = equalized_odds_difference(\n",
    "    y_test,\n",
    "    y_pred_postprocess,\n",
    "    sensitive_features=A_test,\n",
    ")\n",
    "equalized_odds_postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mitigate using Fairlearn Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Understanding the Reductions Approach\n",
    "\n",
    "The Reductions approach, implemented through ExponentiatedGradient, is an in-processing technique that enforces fairness during model training. Key aspects:\n",
    "\n",
    "1. How it works:\n",
    "   - Creates a sequence of reweighted datasets\n",
    "   - Retrains the base classifier on each dataset\n",
    "   - Guaranteed to find a model satisfying fairness constraints while optimizing performance\n",
    "\n",
    "2. Key differences from ThresholdOptimizer:\n",
    "   - Fairness is enforced during training, not after\n",
    "   - Doesn't need sensitive features at prediction time\n",
    "   - Results in a single model rather than group-specific thresholds\n",
    "\n",
    "3. Parameters:\n",
    "   - epsilon: controls the maximum allowed disparity\n",
    "   - Recommended to set epsilon ≈ 1/√(number of samples)\n",
    "\n",
    "The algorithm produces multiple candidate models, allowing us to examine the performance-fairness trade-off and select the most appropriate model for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = ErrorRate(costs={\"fp\": 0.1, \"fn\": 0.9})\n",
    "constraint = EqualizedOdds(difference_bound=0.01)\n",
    "reduction_mitigator = ExponentiatedGradient(model, constraint, objective=objective)\n",
    "reduction_mitigator.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reduction = reduction_mitigator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_reduction = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_reduction,\n",
    "    sensitive_features=A_test,\n",
    ")\n",
    "mf_reduction.overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_reduction.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_reduction.by_group.plot.bar(subplots=True, layout=[1, 3], figsize=[12, 4], legend=None, rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_reduction.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_reduction = balanced_accuracy_score(y_test, y_pred_reduction)\n",
    "balanced_accuracy_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_reduction = equalized_odds_difference(\n",
    "    y_test,\n",
    "    y_pred_reduction,\n",
    "    sensitive_features=A_test,\n",
    ")\n",
    "equalized_odds_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compare our three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_postprocess.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_reduction.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "{balanced_accuracy_unmitigated=}\n",
    "{balanced_accuracy_postprocess=}\n",
    "{balanced_accuracy_reduction=}\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "{equalized_odds_unmitigated=}\n",
    "{equalized_odds_postprocess=}\n",
    "{equalized_odds_reduction=}\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"accuracy\": [\n",
    "            balanced_accuracy_unmitigated,\n",
    "            balanced_accuracy_postprocess,\n",
    "            balanced_accuracy_reduction,\n",
    "        ],\n",
    "        \"equalized_odds\": [\n",
    "            equalized_odds_unmitigated,\n",
    "            equalized_odds_postprocess,\n",
    "            equalized_odds_reduction,\n",
    "        ],\n",
    "    },\n",
    ").plot(x=\"accuracy\", y=\"equalized_odds\", title=\"The Performance-Fairness Trade-Off\")\n",
    "plt.xlabel(\"⬅️ Worse Accuracy                Model Accuracy                Better Accuracy ➡️\")\n",
    "plt.ylabel(\"⬅️ More Fair        Equalized Odds Difference        Less Fair ➡️\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that LightGBM performs best in terms of accuracy. However, high accuracy doesn't mean the model is fair - it might be very accurate at perpetuating historical biases in the training data. This is why we need specialized fairness metrics and mitigation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding the Trade-offs\n",
    "\n",
    "As we can see from the results:\n",
    "- The unmitigated model has the highest accuracy but shows significant bias\n",
    "- The post-processing approach (ThresholdOptimizer) reduces bias but with some cost to accuracy\n",
    "- The reductions approach provides a different balance between fairness and performance\n",
    "\n",
    "In practice, choosing between these approaches depends on:\n",
    "- Your specific fairness requirements\n",
    "- Whether you can access protected characteristics at prediction time\n",
    "- The acceptable trade-off between fairness and performance\n",
    "- Technical constraints of your deployment environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. Machine learning models can easily perpetuate or amplify existing biases\n",
    "2. We need specialized tools and metrics to identify unfairness\n",
    "3. Different mitigation techniques offer different trade-offs\n",
    "4. There's usually no perfect solution - but we can significantly improve fairness\n",
    "\n",
    "Remember: Fairness in ML is an ongoing process, not a one-time fix. Regular monitoring and updates are essential to maintain fair outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fairness-tales-workshop)",
   "language": "python",
   "name": "fairness-tales-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
